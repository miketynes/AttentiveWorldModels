{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import mdn\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, InputLayer, Attention\n",
    "from tensorflow.keras.layers import (Conv2D, Input, Reshape, \n",
    "                                     Lambda, Dense, Conv2DTranspose)\n",
    "\n",
    "if tf.test.is_gpu_available():\n",
    "    physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import utils\n",
    "\n",
    "from tensorflow.keras.callbacks import (EarlyStopping, ModelCheckpoint, \n",
    "                                       TensorBoard, Callback)\n",
    "import datetime\n",
    "from time import time\n",
    "from utils import TrainTimeCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 128\n",
    "act_len = 3\n",
    "n_mixtures = 5\n",
    "output_dims = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!free -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_instances = len(os.listdir('./sausage/states'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_folder(path):\n",
    "    files = sorted(os.listdir(path))\n",
    "    _1 = np.load(os.path.join(path, files[0]))\n",
    "    data = np.zeros((len(files), *_1.shape))\n",
    "    for i, fname in enumerate(files):\n",
    "        data[i] = np.load(os.path.join(path, fname))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_states = load_folder('./sausage/z_states')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = load_folder('./sausage/actions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "154624 / 128 / 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = np.concatenate((z_states, actions), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dataset = tf.data.Dataset.from_tensor_slices(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = latent_dataset.batch(seq_len + 1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_z = chunk[:-1]\n",
    "    target_z = chunk[1:, :32]\n",
    "    return input_z, target_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(10000).batch(utils.BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for a, b in dataset:\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_20 = i // 5\n",
    "val = dataset.take(percent_20)\n",
    "train = dataset.skip(percent_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir \"./logs/\"\n",
    "!mkdir \"./logs/fit\"\n",
    "!rm \"./logs/fit/*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.units = units\n",
    "        self.W1 = tf.keras.layers.Dense(units, input_shape=(35,))\n",
    "        self.W2 = tf.keras.layers.Dense(units, input_shape=(256,))\n",
    "        self.V = tf.keras.layers.Dense(1, input_shape=(256,))\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        # hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        # \n",
    "        features = tf.expand_dims(features, 0)\n",
    "        hidden = tf.expand_dims(hidden, 0)\n",
    "        # score shape == (batch_size, 64, hidden_size)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden))\n",
    "\n",
    "        # attention_weights shape == (batch_size, 64, 1)\n",
    "        # you get 1 at the last axis because you are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        # context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(BahdanauAttention, self).get_config()\n",
    "        config.update({\n",
    "            'units':self.units\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class attention_mdn_rnn(tf.keras.Model):\n",
    "    def __init__(self, \n",
    "                seq_len=128, \n",
    "                act_len=3, \n",
    "                latent_size=32, \n",
    "                cells=256, \n",
    "                output_dim=32, \n",
    "                n_mixes=5):\n",
    "        super(attention_mdn_rnn, self).__init__()\n",
    "\n",
    "        \n",
    "        self.seq_len=seq_len\n",
    "        self.act_len=act_len\n",
    "        self.latent_size=latent_size\n",
    "        self.cells=cells\n",
    "        self.output_dim=output_dim\n",
    "        self.n_mixes=n_mixes\n",
    "        \n",
    "        #self.inputs = Input((None, self.act_len + self.latent_size))\n",
    "        self.lstm   = LSTM(self.cells,\n",
    "                            return_sequences=True,\n",
    "                            return_state=True,\n",
    "                            recurrent_initializer='glorot_uniform')\n",
    "\n",
    "        self.attention = BahdanauAttention(self.cells)\n",
    "        self.out       = mdn.MDN(self.output_dim, self.n_mixes)\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "\n",
    "        context_vector, attention_weights = self.attention(x, hidden)\n",
    "        #context_vector = context_vector.numpy().squeeze()\n",
    "        \n",
    "        # context_vector = features * attention_weights\n",
    "        x, hidden_out, c = self.lstm(context_vector[0]) #remove 1 from input shape (1, x, y, z)\n",
    "        x = self.out(x)\n",
    "        \n",
    "        return x, hidden_out#, attention_weights\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(attention_mdn_rnn, self).get_config()\n",
    "        config.update({'seq_len':self.seq_len,\n",
    "                        'act_len':self.act_len,\n",
    "                        'latent_size':self.latent_size,\n",
    "                        'cells':self.cells,\n",
    "                        'output_dim':self.output_dim,\n",
    "                        'n_mixes':self.n_mixes})\n",
    "        return config\n",
    "\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "    \n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.cells))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = attention_mdn_rnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = mdn.get_mixture_loss_func(32, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_mixes = 5\n",
    "output_dim = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(pair, target):\n",
    "    loss = 0\n",
    "    hidden = M.reset_state(128)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        z, hidden = M(pair, hidden)\n",
    "        \n",
    "        try:\n",
    "            loss += loss_function(target, z)\n",
    "        except Exception as e:\n",
    "            import pdb; pdb.set_trace()\n",
    "    \n",
    "    \n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = M.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (z_tensor, target)) in enumerate(train):\n",
    "        batch_loss, t_loss = train_step(z_tensor, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "#         if batch % 5 == 0:\n",
    "#             print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "#               epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
    "\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / i)\n",
    "\n",
    "    for (batch, (z_tensor, target)) in enumerate(val):\n",
    "        batch_loss, t_loss = train_step(z_tensor, target)\n",
    "        total_loss += t_loss\n",
    "        \n",
    "    val_loss.append(total_loss / i)\n",
    "\n",
    "    # print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
    "    #                                      total_loss/i))\n",
    "    # print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M.save_weights('./data/weights/attn_mdn_rnn', save_format='tf') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
