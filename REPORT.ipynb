{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $$\\text{Attentive World Models}$$\n",
    "\n",
    "$$\\text{Aaron Dharna       | Michael Tynes}$$\n",
    "$$\\text{Fordham University | Fordham University}$$\n",
    "$$\\text{adharna@fordham.edu| mtynes@fordham.edu}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       "    if (code_show){\n",
       "        $('div.cell.code_cell.rendered.selected div.input').hide();\n",
       "    } else {\n",
       "        $('div.cell.code_cell.rendered.selected div.input').show();\n",
       "    }\n",
       "    code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "To show/hide this cell's raw code input, click <a href=\"javascript:code_toggle()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "from IPython.display import display\n",
    "\n",
    "# Taken from https://stackoverflow.com/questions/31517194/how-to-hide- \\\n",
    "# one-specific-cell-input-or-output-in-ipython-notebook\n",
    "\n",
    "tag = HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    "    if (code_show){\n",
    "        $('div.cell.code_cell.rendered.selected div.input').hide();\n",
    "    } else {\n",
    "        $('div.cell.code_cell.rendered.selected div.input').show();\n",
    "    }\n",
    "    code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "To show/hide this cell's raw code input, click <a href=\"javascript:code_toggle()\">here</a>.''')\n",
    "display(tag)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from models import *\n",
    "from utils import *\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: I have started putting images for us to load into `AWM/images`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "### RL\n",
    "\n",
    "----  \n",
    "\n",
    "## Models\n",
    "### VAE\n",
    "    - To accomplish the Vision task needed for CarRacing-V0, we used a Convolutional Variational Auto-encoder (VAE). The VAE is filling in the task of compressing our image-state representations into a 32-dimensional vector, z. This latent vector representation should now be representative of the input image. If this is the case, then we have managed to take a 64x64 dimensional object and embed it into a 32 dimensional subspace without losing relavent informatino about the input. In fact, we can check that by recreating a new image from the z-space and seeing that the important information hass been preserved. \n",
    "    \n",
    "    - input reconstruction images.\n",
    "    \n",
    "    - Furthermore, due to the VAE being a generative model, we can dream up completely new images by simply injecting some noise into the bottleneck layer. \n",
    "    \n",
    "    - In this manner we have managed to compress the spatial information of CarRacing. However, that is not all that is required to solve this task. We must also learn temporal information. \n",
    "    \n",
    "### MDN-RNN\n",
    "\n",
    "    - \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "### Controller\n",
    "\n",
    "    - The controller, C, interacts with the environment. Since we have gone through all of the work of learning V and M, we can use a simple function approximator to handle the control -- in fact, C is linear transformation upon z (the compressed image) and h (the compressed temporal information). \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(288, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = Controller(256+32, 3)\n",
    "C.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----  \n",
    "\n",
    "## Methodology\n",
    "### Data collection\n",
    "    - What's our dataset?\n",
    "Our dataset, $\\mathcal{D}$, is play-traces of the CarRacing_v0 environment in OpenAI Gym. These traces combined states and actions. (If we were using an RL algorithm to teach C, then we would also record reward information at each step.) We recorded 500 playthroughs of the CarRacing_v0 environment where each playthrough was 384 steps long. This lead to a total of 192000 frames recorded. Each frame of $\\mathcal{D}$ was resized from `400x600x3` to a `64x64x3` image. After training $\\mathcal{V}$ with $\\mathcal{D}$ we then applied  $\\mathcal{V}(\\mathcal{D})$ such that we now also recoreded `z_states`. When we combined $\\mathcal{V}(\\mathcal{D})$ with $\\mathcal{D}$ we created a new dataset, $\\mathcal{D'}$. These z_states are 32-dimensional latent representation of the images extracted from V's bottleneck. $\\mathcal{D'}$ is then used to train $\\mathcal{M}$ and $\\mathcal{C}$.\n",
    "    \n",
    "### Training\n",
    "    - Each model was trained individually.  \n",
    "    - $\\mathcal{V}$ was trained to minimize the evidence lower bound.\n",
    "    - $\\mathcal{M}$ was trained to minimize the negative log-likelihood between $\\mathcal{V}$ encoded states and samples from factoried gaussian parameters output by the MDN-head. \n",
    "    - $\\mathcal{C}$ was trained using an evolutionary algorithm that maximizes the fitness of a population of linear-controllers using a non-gradient optimization method.\n",
    "    \n",
    "    \n",
    "### Attention\n",
    "### Evolution\n",
    "\n",
    "----\n",
    "\n",
    "## Results\n",
    "### VAE\n",
    "\n",
    "### MDN-RNN\n",
    "#### CONTROLLER\n",
    "\n",
    "### ATTN-MDN-RNN\n",
    "#### CONTROLLER\n",
    "\n",
    "----  \n",
    "\n",
    "## Discussion\n",
    "### Where did this go wrong?\n",
    "### What's next?\n",
    "### Profit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1)  worldmodels.github.io, Ha and Schmidhuber 2018  \n",
    "(2) Xu, K., Ba, J., Kiros, R., et al. **Show, Attend, and Tell**, 2015, arXiv e-prints, arXiv:1502.03044\n",
    "(3) \n",
    "(4) \n",
    "(5) \n",
    "(6) \n",
    "(7) \n",
    "(8) \n",
    "(9) \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
