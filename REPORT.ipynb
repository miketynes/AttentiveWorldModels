{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $$\\text{Attentive World Models}$$\n",
    "\n",
    "$$\\text{Aaron Dharna       | Michael Tynes}$$\n",
    "$$\\text{Fordham University | Fordham University}$$\n",
    "$$\\text{adharna@fordham.edu| mtynes@fordham.edu}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       "    if (code_show){\n",
       "        $('div.cell.code_cell.rendered.selected div.input').hide();\n",
       "    } else {\n",
       "        $('div.cell.code_cell.rendered.selected div.input').show();\n",
       "    }\n",
       "    code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "To show/hide this cell's raw code input, click <a href=\"javascript:code_toggle()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "from IPython.display import display\n",
    "\n",
    "# Taken from https://stackoverflow.com/questions/31517194/how-to-hide- \\\n",
    "# one-specific-cell-input-or-output-in-ipython-notebook\n",
    "\n",
    "tag = HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    "    if (code_show){\n",
    "        $('div.cell.code_cell.rendered.selected div.input').hide();\n",
    "    } else {\n",
    "        $('div.cell.code_cell.rendered.selected div.input').show();\n",
    "    }\n",
    "    code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "To show/hide this cell's raw code input, click <a href=\"javascript:code_toggle()\">here</a>.''')\n",
    "display(tag)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from models import *\n",
    "from utils import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: I have started putting images for us to load into `AWM/images`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "### Control in MDPs\n",
    "\n",
    "Markov decision processes allow us to model control tasks in a concise way. We have an agent interacting with an environment that consists of a set of states $\\mathcal S$, a set of actions that can be performed in those states $\\mathcal A$, and a scalar reward associated with each state $R(s)$.\n",
    "\n",
    "We assume ethat there exists a funciton $p(s_{t+1} | s_t, a_t)$ that fully describes the dynamics of the environment. Thus it is Markovian. \n",
    "\n",
    "| ![](images/MDP.png)|\n",
    "|:----:|\n",
    "|Sutton and Barto, 2018|\n",
    "\n",
    "An environment is 'solved' by an agent when it finds a policy for aciton $\\pi(a_t | s_t)$ maximize expected long term reward, which is defined as  \n",
    "\n",
    "$$G_t = R_t + \\gamma R_{t+1} + \\gamma^2R_{t+2} + \\cdots$$\n",
    "for $\\gamma \\in [0, 1]$\n",
    "\n",
    "Thus our optimal policy $\\pi_*$ is defined as \n",
    "$$\\pi_* = \\arg \\max_\\pi  \\mathbb E [G_t | s_t, a_t] \\forall s$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## $$\\text{Models}$$\n",
    "### VAE\n",
    "\n",
    "To accomplish the Vision task needed for CarRacing-V0, we (and Ha and Schmidhuber) used a Convolutional Variational Auto-encoder (VAE). The VAE is filling in the task of compressing our image-state representations into a 32-dimensional vector, z. This latent vector representation should now be representative of the input image. If this is the case, then we have managed to take a 64x64 dimensional object and embed it into a 32 dimensional subspace without losing relavent informatino about the input. In fact, we can check that by recreating a new image from the z-space and seeing that the important information hass been preserved. \n",
    "    \n",
    "|![](images/vae_recon_input.png) | ![](images/vae_recon_output.png)|\n",
    "|:----:|:----:|\n",
    "| original   | reconstruction   |\n",
    "    \n",
    "Furthermore, due to the VAE being a generative model, we can dream up completely new images by simply injecting some noise into the bottleneck layer. \n",
    "\n",
    "|![](images/vae_dream_image.png) |\n",
    "|:----:|\n",
    "| dream image|\n",
    "    \n",
    "In this manner we have managed to compress the spatial information of CarRacing. However, that is not all that is required to solve this task. We must also learn temporal information. \n",
    "    \n",
    "### MDN-RNN\n",
    "\n",
    "RNN models the dynamics of the world, learning the mappings from ((compresed) state, action) pairs to next states.\n",
    "Assuming the VAE does its job of comopressing the states well, we essentially learn the function\n",
    "$$f: s_t, a_t \\rightarrow s_{t+1}$$ which is an essential property if MDPs. But of course, MDPs are stochastic by definition, which is where the MDN proves useful, allowing us to learn a distribution of next states given a state, action pair, that is: \n",
    "$$p(s_{t+1} | s_t, a_t)$$\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "### Controller\n",
    "\n",
    "    - The controller, C, interacts with the environment. Since we have gone through all of the work of learning V and M, we can use a simple function approximator to handle the control -- in fact, C is linear transformation upon z (the compressed image) and h (the compressed temporal information). \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(288, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = Controller(256+32, 3)\n",
    "C.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Gym  \n",
    "Simulation API for many different (and difficult) control tasks. We chose to work in `CarRacing_v0` environment as that is the same environment that Ha uses. Ha's World Model's solution was the first automated system to sucessfully solve this task.  In this API, we have access to our environment, recieve rewards and states, and can pass in new actions for the agent to take. \n",
    "\n",
    "```python\n",
    "def rollout(controller):\n",
    "  ''' env, rnn, vae are '''\n",
    "  ''' global variables  '''\n",
    "  obs = env.reset()\n",
    "  h = rnn.initial_state()\n",
    "  done = False\n",
    "  cumulative_reward = 0\n",
    "  while not done:\n",
    "    z = vae.encode(obs)\n",
    "    a = controller.action([z, h])\n",
    "    obs, reward, done = env.step(a)\n",
    "    cumulative_reward += reward\n",
    "    h = rnn.forward([a, z, h])\n",
    "  return cumulative_reward\n",
    "```\n",
    "\n",
    "| ![](images/CarRacing.gif)|\n",
    "|:----:|\n",
    "|Ha and Schmidhuber 2018|\n",
    "\n",
    "CarRacing-v0 defines “solving” as getting average reward of 900 over 100 consecutive trials, which means the agent can only afford very few driving mistakes. [OpenAI](https://github.com/openai/gym/blob/master/gym/envs/box2d/car_racing.py)\n",
    "\n",
    "----  \n",
    "\n",
    "## Methodology\n",
    "### Data collection\n",
    "    - What's our dataset?\n",
    "Our dataset, $\\mathcal{D}$, is play-traces of the CarRacing_v0 environment in OpenAI Gym. These traces combined states and actions. (If we were using an RL algorithm to teach C, then we would also record reward information at each step.) We recorded 500 playthroughs of the CarRacing_v0 environment where each playthrough was 384 steps long. This lead to a total of 192000 frames recorded. Each frame of $\\mathcal{D}$ was resized from `400x600x3` to a `64x64x3` image. After training $\\mathcal{V}$ with $\\mathcal{D}$ we then applied  $\\mathcal{V}(\\mathcal{D})$ such that we now also recoreded `z_states`. When we combined $\\mathcal{V}(\\mathcal{D})$ with $\\mathcal{D}$ we created a new dataset, $\\mathcal{D'}$. These z_states are 32-dimensional latent representation of the images extracted from V's bottleneck. $\\mathcal{D'}$ is then used to train $\\mathcal{M}$ and $\\mathcal{C}$.\n",
    "\n",
    "There was a surprising amount of difficult getting good traces to learn from. \n",
    "    \n",
    "### Training\n",
    "Each model was trained individually.\n",
    "$\\mathcal{V}$ was trained to minimize the evidence lower bound.\n",
    "$\\mathcal{M}$ was trained to minimize the negative log-likelihood between $\\mathcal{V}$ encoded states and samples from factoried gaussian parameters output by the MDN-head. \n",
    "$\\mathcal{C}$ was trained using an evolutionary algorithm that maximizes the fitness of a population of linear-controllers using a non-gradient optimization method.\n",
    "    \n",
    "    \n",
    "### Attention\n",
    "### Evolution\n",
    "\n",
    "Evolution is an alternatve approach to reinforcement learning when learning a control task. Evolution as an optimization paradigm draws inspiration from natute. In 'normal' optimization, we have a singular function approximator, $\\mathcal F$ whose parameters we alter until it learns to fit some given dataset. In gradient based optimization, we usually find a optimum by taking a derivative of a loss function with respect to the parameters of $\\mathcal F$, setting that to zero, and then editing $\\mathcal F$'s parameters in the direction of greatest descent. In an evolutionary paradigm we do not do this. \n",
    "\n",
    "First, we build a population of potential candidate solutions $\\mathcal P$. We then evaluate each $p \\in \\mathcal P$ on the task at hand receiving a score of p's performace. After ranking each p $\\in \\mathcal P$ we then pick the best performing individuals and create new candidate solutions from them. Since the next generation is created from the best performing individuals of the previous generation, the behavior of the individuals should keep improving. \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;35mattn_mdn_rnn_learning_curves.png\u001b[0m  \u001b[01;35mrollout.png\u001b[0m           \u001b[01;35mvae_training_loss.png\u001b[0m\r\n",
      "\u001b[01;35mCarRacing.gif\u001b[0m                     \u001b[01;35mvae_dream_image.png\u001b[0m   \u001b[01;35mvae_val_loss.png\u001b[0m\r\n",
      "\u001b[01;35mmdn_rnn_learning_curves.png\u001b[0m       \u001b[01;35mvae_recon_input.png\u001b[0m   \u001b[01;35mworld_model.png\u001b[0m\r\n",
      "\u001b[01;35mMDP.png\u001b[0m                           \u001b[01;35mvae_recon_output.png\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "ls images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $$\\text{Results}$$\n",
    "### VAE\n",
    "Vae training was successful. See vae_training.ipynb (and the associated HTML) for more. \n",
    "\n",
    "|![](images/vae_training_loss.png)|![](images/vae_val_loss.png)|\n",
    "|:---:|:---:|\n",
    "|Train loss| Val loss|\n",
    "\n",
    "### MDN-RNN\n",
    "Training for the non-attentive version of the MDN-RNN appears to have also been successful. Interestingly, the val and train loss track one another almost perfectly. This may be due to strong performance in modelling the world dynamics. It does look good enough to arouse some suspicion, but we haven't been able to find anything wrong with the code...  \n",
    "\n",
    "|![](images/mdn_rnn_learning_curves.png)|\n",
    "|:---:|\n",
    "|MDN-RNN learning curves|\n",
    "\n",
    "### ATTN-MDN\n",
    "\n",
    "The attentive MDN-RNN learning curves are a little more erratic, although they still seem to converge. A random hyperparameter search could calm the loss time dynamics.\n",
    "\n",
    "|![](images/attn_mdn_rnn_learning_curves.png)|\n",
    "|:---:|\n",
    "|ATTN-MDN-RNN learning curves|\n",
    "\n",
    "\n",
    "#### CONTROLLER\n",
    "\n",
    "----  \n",
    "\n",
    "## Discussion\n",
    "### Where did this go wrong?\n",
    "### What's next?\n",
    "### Profit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1)  worldmodels.github.io, Ha and Schmidhuber 2018  \n",
    "(2) Xu, K., Ba, J., Kiros, R., et al. **Show, Attend, and Tell**, 2015, arXiv e-prints, arXiv:1502.03044\n",
    "(3) \n",
    "(4) \n",
    "(5) \n",
    "(6) \n",
    "(7) \n",
    "(8) \n",
    "(9) \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
