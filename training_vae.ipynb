{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize\n",
    "from tensorflow.keras.callbacks import (EarlyStopping, ModelCheckpoint, \n",
    "                                       TensorBoard, Callback)\n",
    "import datetime\n",
    "from time import time\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "physical_devices\n",
    "\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\"\"\"\n",
    "The above line comes from here: \n",
    "https://github.com/tensorflow/tensorflow/blob/6e559b96c8146ce15c7c03f66e515e31a6b0aa00/tensorflow/python/framework/config.py#L443\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Conv2D, Input, Reshape, \n",
    "                                     Lambda, Dense, Conv2DTranspose)\n",
    "\n",
    "IMG_HEIGHT = 64\n",
    "IMG_WIDTH  = 64\n",
    "\n",
    "LATENT_SIZE = 32\n",
    "BATCH_SIZE  = 128\n",
    "KL_TOLERANCE = 0.5\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
    "\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "    \n",
    "    # import pdb; pdb.set_trace()\n",
    "    z_mean, z_log_var = args\n",
    "    #_batch = z_mean.shape[0]\n",
    "    #_dim = z_mean.shape[1]\n",
    "    batch = BATCH_SIZE# if _batch is None else _batch\n",
    "    dim = LATENT_SIZE #if _dim is None else _dim\n",
    "    # by default, random_normal has mean = 0 and std = 1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    foo = z_mean + K.exp(0.5 * z_log_var)# * epsilon\n",
    "    # print(foo.shape)\n",
    "    bar = foo * epsilon\n",
    "    # print(bar.shape)\n",
    "    return bar\n",
    "\n",
    "\n",
    "## ENCODER\n",
    "\n",
    "inputs = Input(shape=(64, 64, 3), name='encoder_input')\n",
    "h = Conv2D(32, 4, strides=2, activation=\"relu\", name=\"enc_conv1\")(inputs)\n",
    "h = Conv2D(64, 4, strides=2, activation=\"relu\", name=\"enc_conv2\")(h)\n",
    "h = Conv2D(128, 4, strides=2, activation=\"relu\", name=\"enc_conv3\")(h)\n",
    "h = Conv2D(256, 4, strides=2, activation=\"relu\", name=\"enc_conv4\")(h)\n",
    "h = Reshape([2*2*256])(h)\n",
    "z_mean = Dense(LATENT_SIZE, name='z_mean')(h)\n",
    "z_log_var = Dense(LATENT_SIZE, name='z_log_var')(h)\n",
    "z = Lambda(sampling, output_shape=(LATENT_SIZE,), name='z')([z_mean, z_log_var])\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "\n",
    "\n",
    "encoder.summary()\n",
    "\n",
    "## DECODER\n",
    "\n",
    "latent_inputs = Input(shape=(LATENT_SIZE,), name='decoder_input')\n",
    "h = Dense(4*256, name=\"dec_fc\")(latent_inputs)\n",
    "h = Reshape([1, 1, 4*256])(h)\n",
    "h = Conv2DTranspose(128, 5, strides=2, activation=\"relu\", name=\"dec_deconv1\")(h)\n",
    "h = Conv2DTranspose(64, 5, strides=2, activation=\"relu\", name=\"dec_deconv2\")(h)\n",
    "h = Conv2DTranspose(32, 6, strides=2, activation=\"relu\", name=\"dec_deconv3\")(h)\n",
    "outputs = Conv2DTranspose(3, 6, strides=2, activation='sigmoid', name=\"dec_deconv4\")(h)\n",
    "\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(decoder, n=128):\n",
    "    z = tf.random.normal(shape=(n, LATENT_SIZE))\n",
    "    return decoder.predict(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = Model(inputs, outputs, name='SpatialWorldModel')\n",
    "\n",
    "## Loss stuff\n",
    "\n",
    "# loss\n",
    "eps = 1e-6 # avoid taking log of zero\n",
    "\n",
    "# reconstruction loss\n",
    "r_loss = tf.reduce_sum(\n",
    "  tf.square(inputs - outputs),\n",
    "  axis = [1,2,3]\n",
    ")\n",
    "r_loss = tf.reduce_mean(r_loss)\n",
    "\n",
    "# augmented kl loss per dim (axis may need to change)\n",
    "kl_loss = - 0.5 * tf.reduce_sum(\n",
    "  (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)),\n",
    "  axis = 1\n",
    ")\n",
    "# todo: look this up. why did Ha do it this way?\n",
    "kl_loss = tf.maximum(kl_loss, KL_TOLERANCE * LATENT_SIZE)\n",
    "kl_loss = tf.reduce_mean(kl_loss)\n",
    "\n",
    "loss = r_loss + kl_loss\n",
    "\n",
    "vae.add_loss(loss)\n",
    "vae.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Small is data fit: can we do well with 5 gigs train, 1 gig val, 1 gig test?\n",
    "\n",
    "# This way we safely avoid data loaders\n",
    "\n",
    "data_dir = \"./data/kaggle/\"\n",
    "\n",
    "#def get_data_dir_size(data_dir):\n",
    "\"\"\"Assuming fixed element size\"\"\"\n",
    "alldata = os.listdir(data_dir)\n",
    "elem = np.load(os.path.join(data_dir, alldata[0]))\n",
    "elem_size = elem.__sizeof__()\n",
    "n_elems = len(alldata)\n",
    "bytes_to_gigs = 1e-9\n",
    "\n",
    "total_data_gigs = elem_size * n_elems * bytes_to_gigs\n",
    "    #return total_data_gigs\n",
    "\n",
    "total_data_gigs\n",
    "\n",
    "# !du -h /kaggle/input/screenshots/kaggle \n",
    "\n",
    "#n_elems_for_m_gigs = lambda m_gigs: m_gigs // (elem_size * bytes_to_gigs)\n",
    "#n_elems_for_7_gigs = n_elems_for_m_gigs(7)\n",
    "\n",
    "alldata = os.listdir(data_dir)\n",
    "np.random.shuffle(alldata)\n",
    "test = 1/8\n",
    "\n",
    "train_ids, test_ids = alldata[int(len(alldata)*test):], alldata[:int(len(alldata)*test)]\n",
    "\n",
    "def read_data(data_dir, file_IDs):\n",
    "    t = np.zeros((len(file_IDs), 64, 64, 3), dtype=np.float32)\n",
    "    for i, file in enumerate(file_IDs):\n",
    "        t[i] = np.load(os.path.join(data_dir, file))\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!free -m "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = read_data(data_dir, train_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = read_data(data_dir, test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(train_images, test_size=256*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Real boii fitting\n",
    "\n",
    "class TrainTimeCallback(Callback): \n",
    "    def __init__(self): \n",
    "        super(TrainTimeCallback, self).__init__()\n",
    "        \n",
    "    def on_train_begin(self, logs=None): \n",
    "        self._start_time = time()\n",
    "        \n",
    "    def on_train_end(self, logs=None):\n",
    "        self._train_time = time() - self._start_time\n",
    "        \n",
    "    @property\n",
    "    def train_time(self):\n",
    "        s = self._train_time\n",
    "        hours, remainder = divmod(s, 3600)\n",
    "        minutes, seconds = divmod(remainder, 60)\n",
    "        \n",
    "        return f'{hours:02.0f}:{minutes:02.0f}:{seconds:02.0f}'\n",
    "    \n",
    "    def print_train_time(self):\n",
    "        print(f'Train time for model: {self.train_time}')\n",
    "\n",
    "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "monitor = 'loss' # change this\n",
    "callbacks = [\n",
    "    TrainTimeCallback(),\n",
    "    ModelCheckpoint('./best_VAE.h5', save_best_only=True, \n",
    "                    monitor=monitor),\n",
    "    EarlyStopping(monitor, patience=10,\n",
    "                  mode='min',\n",
    "                  restore_best_weights=True),\n",
    "    TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!free -m "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = vae.fit(train_images,\n",
    "                  epochs=100, \n",
    "                  batch_size=BATCH_SIZE,\n",
    "                  callbacks=callbacks, \n",
    "                  validation_data=(val, None))\n",
    "    \n",
    "callbacks[0].print_train_time()\n",
    "\n",
    "with open('history.json', 'w+') as f:\n",
    "    f.write(json.dumps(history.history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = sample(vae.layers[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(im[8])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
